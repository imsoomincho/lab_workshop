{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "W6_Chapter04_Training_of_Neural_Net.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yoonlee78/lab_workshop/blob/master/W7/W7_Chapter04_Training_of_Neural_Net_part1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2SzAKFb_M9le",
        "colab_type": "text"
      },
      "source": [
        "# Deep Learning from scratch\n",
        "\n",
        "## Chapter 04. 신경망 학습 Part 1\n",
        "\n",
        "### 4.1. 데이터에서 학습한다!\n",
        "\n",
        "신경망의 특징은 데이터를 보고 학습할 수 있다는 점이다. 데이터에서 학습한다는 것은 **가중치 매개변수**의 값을 데이터를 보고 자동으로 결정한다는 뜻이다. \n",
        "\n",
        "2장의 퍼셉트론 예에서는 진리표를 보면서 사람이 수작업으로 매개변수 값을 설정함. 하지만 이때는 매개변수가 겨우 3개였음. 그렇다면 실제 신경망에서는 매개 변수가 수천에서 수만가지까지 된다. 나아가 층을 깊게 한 딥러닝 정도 되면 그 수는 수억에 이를 수도 있다. 이쯤 되면 (아니 훨씬 전부터) 매개 변수를 수작업으로 정한다는 것은 아예 불가능하다. 이번 장에서는 신경망 학습 (데이터로부터 매개변수의 값을 정하는 방법)에 대해서 설명하고 파이썬으로 MNIST 데이터셋의 손글씨 숫자를 학습하는 코드를 구현해보자. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KTqahcY3O_Re",
        "colab_type": "text"
      },
      "source": [
        "### 4.1.1. 데이터 주도 학습\n",
        "\n",
        "기계학습은 데이터가 생명이다. 데이터에서 답을 찾고 패턴을 발견하고 이야기를 만든다. \n",
        "\n",
        "기계학습에서는 사람의 개입을 최소화하고 수집한 데이터로부터 패턴을 찾고자 한다. \n",
        "\n",
        "신경망과 딥러닝은 기존 기계학습에서 사용하던 방법보다 사람의 개입을 더욱 배제할 수 있게 해주는 중요한 특성을 지녔다. \n",
        "\n",
        "구체적인 문제를 하나 더 생각해보자면 이미지에서 '5'라는 숫자를 인식하는 프로그램을 구현한다고 했을때, [그림 4-1]과 같은 자유분방한 손글씨 이미지를 보고 5인지 아닌지를 알아보는 프로그램을 구현하는 것이 목표다. '5'를 인식하는 대표 알고리즘을 밑바닥부터 설계하는 것은 어렵다, 대신 주어진 데이터를 잘 활용해서 해결하게 하면 된다. 그런 방법의 하나로, 이미지에서 **특징**<sup>feature</sup>을 추출하고 그 특징의 패턴을 기계학습 기술로 학습하는 방법이 있다. 여기서 말하는 특징은 입력 데이터 (입력 이미지)에서 본질적인 데이터 (중요한 데이터)를 정확하게 추출할 수 있도록 설계된 변환기를 가리킨다. 이미지의 특징은 보통 벡터로 기술하고 컴퓨터 비전 분야에서는 SIFT, SURF, HOG 등의 특징을 많이 사용한다. \n",
        "\n",
        "이런 특징을 사용하여 이미지 데이터를 벡터로 변환하고, 변환된 벡터를 가지고 지도 학습 방식의 대표 분류 기법인 SVM, KNN 등으로 학습할 수 있다. \n",
        "\n",
        "### 그림 4-2. 규칙을 '사람'이 만드는 방식에서  '기계'가 데이터로 배우는 방식으로의 패러다임 전환: 회색 블록 = 사람이 개입하지 않음'\n",
        "\n",
        "딥러닝을 **종단간 기계학습: end-to-end machine learning** 이라고도 한다. 여기서 종단간은 '처음부터 끝가지'라는 의미로, 데이터(입력)에서 목표한 결과(출력)를 사람의 개입 없이 얻는다는 뜻을 담고 있다. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "muLzS68zT2vw",
        "colab_type": "text"
      },
      "source": [
        "### 4.1.2. 훈련 데이터와 시험 데이터\n",
        "\n",
        "기계학습 문제는 데이터를 **훈련 데이터**training data와 **시험 데이터**test data로 나누어 학습과 실험을 수행하는 것이 일반적이다. \n",
        "\n",
        "### 4.2.1. 평균 제곱 오차\n",
        "\n",
        "가장 많이 쓰이는 손실 함수는 **평균 제곱 오차**mean squared error, MSE이다. 평균 제곱 오차는 수식으로는 다음과 같다. \n",
        "\n",
        "- 수식. 4.1. \n",
        "\n",
        "여기서 y<sub>k</sub>는 신경망의 출력 (신경망이 추정한 값), t<sub>k</sub>은 정답 레이블, k는 데이터 차원 수를 나타낸다. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R97-GuoSM1pp",
        "colab_type": "text"
      },
      "source": [
        "\\begin{equation*}\n",
        "E = \\frac{1}{2} \\sum_{k} (y_{k}-t_{k})^{2}\n",
        "\\end{equation*}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0MKYeCToM6f8",
        "colab_type": "text"
      },
      "source": [
        "y<sub>k</sub> : 신경망의 출력(신경망이 추정한 값) <br>\n",
        "t<sub>k</sub> : 정답 레이블 <br>\n",
        "k :  데이터의 차원 수 <br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EK7UNyM8NBWc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]\n",
        "t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ADAPiYcbNJel",
        "colab_type": "text"
      },
      "source": [
        "배열들의 원소는 첫 번째 인덱스부터 순서대로 숫자 '0', '1', '2', ...일 때 값\n",
        "\n",
        "y = 신경망의 출력 =  소프트맥스 함수의 출력\n",
        "\n",
        "t 정답을 의미하는 위치의 원소는 1로, 그 외에는 0으로 표기\n",
        "\n",
        "소프트맥스 함수의 출력은 확률로 해석할 수 있으므로 이 예에서는 이미지가 0일 확률은 0.1, '1'일 확률은 0.05, '2'일 확률은 0.6이라고 해석됨. \n",
        "\n",
        "[지난 시간 복습] **원-핫 인코딩**: 한 원소만 1로 하고 그 외는 0으로 나타내는 표기법\n",
        "\n",
        "숫자 '2'에 해당하는 원소의 값이 1이므로 정답이 '2'임을 알 수 있다. \n",
        "\n",
        "평균 제곱 오차 파이썬 구현"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7yQ7-PIlNrII",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def mean_squared_error(y, t):\n",
        "  return 0.5 * np.sum((y-t)**2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "umRnQtafNyQ_",
        "colab_type": "text"
      },
      "source": [
        "여기서 인수 y와 t는 넘파이 배열이다. 이 코드 [4.1]을 그대로 구현한 것이니 설명은 생략한다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k-GcGGC4OWrb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y0QjfzVgN6EC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#정답은 '2'\n",
        "\n",
        "t = [0,0,1,0,0,0,0,0,0,0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QanQnlaHOBab",
        "colab_type": "code",
        "outputId": "e83726c6-7a19-42e2-81ce-d660a1ad3872",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#예1: '2'일 확률이 가장 높다고 추정함 (0.6)\n",
        "y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]\n",
        "mean_squared_error(np.array(y), np.array(t))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.09750000000000003"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C_HU4tKbOVHF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#예2 : '7'일 확률이 가장 높다고 추정함 (0.6)\n",
        "y = [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q-58ME5eOh_n",
        "colab_type": "code",
        "outputId": "26bd9426-064e-4988-f462-e50d29a98285",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "mean_squared_error(np.array(y), np.array(t))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5975"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1gmMDT0yOlin",
        "colab_type": "text"
      },
      "source": [
        "위 두 가지 예시에서 첫 번째의 예의 정답은 '2'이고 신경망의 출력도 '2'에서 가장 높은 경우이다. 두 번째 예에서는 정답은 똑같이 '2'지만, 신경망의 출력은 '7'에서 가장 높다.\n",
        "\n",
        "이 실험의 결과로 첫 번째 예의 손실 함수 쪽 출력이 작으며 정답 레이블과의 오차도 작은 것을 알 수 있다. 즉, 평균 제곱 오차를 기준으로는 첫 번째 추정 결과가 (오차가 더 작으니) 정답에 더 가까울 것으로 판단할 수 있다. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "daGyAJ54O8AV",
        "colab_type": "text"
      },
      "source": [
        "### 4.2.2. 교차 엔트로피 오차 \n",
        "\n",
        "또 다른 손실 함수로서 **교차 엔트로피 오차**(cross entropy error)도 자주 이용한다. \n",
        "\n",
        "\\begin{equation*}\n",
        "E = - \\sum_{k} t_{k} log y_{k}\n",
        "\\end{equation*}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kH0r8FVZPJZv",
        "colab_type": "text"
      },
      "source": [
        "여기에서 log는 밑이 e인 자연로그(log<sub>e</sub>)이다.\n",
        "\n",
        "y<sub>k</sub>는 신경망의 출력, t<sub>k</sub>는 정답 레이블이다. \n",
        "\n",
        "또, t<sub>k</sub>는 정답에 해당하는 인덱스의 원소만 1이고 나머지는 0이다 (원-핫 인코딩).\n",
        "\n",
        "그래서 식[4.2]는 실질적으로 정답일 때의 추정 (t<sub>k</sub>가 1일 때의  y<sub>k</sub>)의 자연로그를 계산하는 식이 된다. \n",
        "\n",
        "예를 들어 정답 레이블은 '2'가 정답이라 하고 이때의 신경망 출력이 0.6이라면 교차 엔트로피 오차는 -log0.6 = 0.51이 된다. 또한, 같은 조건에서 신경망 출력이 0.1이라면 -log0.1=2.30이 된다. 즉, 교차 엔트로피 오차는 정답일 때의 출력이 전체 값을 정하게 된다.\n",
        "\n",
        "한편, [그림4-3]은 자연로그의 그래프이다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IAMmH9zYP24J",
        "colab_type": "code",
        "outputId": "05bca5a9-6811-4316-8190-817369872b47",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pylab as plt\n",
        "\n",
        "x = np.arange(0.001, 1.0, 0.001)\n",
        "y = np.log(x)\n",
        "plt.plot(x, y)\n",
        "plt.ylim(-5.0, 0.0) # y축의 범위 지정\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAD8CAYAAABjAo9vAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZREExICCP6H-",
        "colab_type": "text"
      },
      "source": [
        "이 그림에서 보듯이 x가 1일 때, y는 0이 되고 x가 0에 가까워질수록 y의 값은 점점 작아진다. \n",
        "\n",
        "[식 4.2]도 마찬가지로 정답에 해당하는 출력이 커질수록 0에 다가가다가, 그 출력이 1일 때 0이 된다. 반대로 정답일 때의 출력이 작아질수록 오차는 커진다. \n",
        "\n",
        "그럼 교차 엔트로피 오차를 구해보자 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yDp4J8x8QKsm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cross_entropy_error(y, t):\n",
        "  delta = 1e-7\n",
        "  return -np.sum(t* np.log(y + delta))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nv70Ldg8QSKn",
        "colab_type": "text"
      },
      "source": [
        "여기서 y와 t는 넘파이 배열이다. **단, ** 코드 마지막에 np.log를 계산할 때 아주 작은 값인 delta를 더했다. 이는 no.log()함수에 0을 입력하면 마이너스 무한대를 뜻하는 -inf가 되어 더 이상 계산을 진행할 수 없게 되기 때문이다. 아주 작은 값을 더해서 절대 0이 되지 않도록, 즉 마이너스 무한대가 발생하지 않도록 한 것. 그러면 이 cross_entropy_error(y, t)함수를 써서 간단한 계산을 해보자 (정답은 똑같이 2이다)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Df1DE-l1QqNu",
        "colab_type": "code",
        "outputId": "4e3ed3e5-b920-47ea-a31e-16fbb24f7ade",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
        "\n",
        "y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]\n",
        "\n",
        "cross_entropy_error(np.array(y), np.array(t))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.510825457099338"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ktJG75t0Q1vc",
        "colab_type": "code",
        "outputId": "3d12a269-a5b4-4d90-a097-7b95710a824a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "y = [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0]\n",
        "cross_entropy_error(np.array(y), np.array(t))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2.302584092994546"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YO5gSnzvQ8n0",
        "colab_type": "text"
      },
      "source": [
        "첫 번째 예는 정답일 때의 출력이 0.6인 경우로, 이때의 교차 엔트로피 오차는 약 0.51이다. 그 다음은 정답일 때의 출력이 (더 낮은) 0.1인 경우로, 이때의 교차 엔트로피 오차는 무려 2.3이다. 즉, 결과 (오차 값)가 더 작은 첫 번째 추정이 정답일 가능성이 높다고 판단한 것으로, 앞서 평균 제곱 오차의 판단과 일치하다. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1UPqC679RKuP",
        "colab_type": "text"
      },
      "source": [
        "### 4.2.3. 미니배치 학습\n",
        "\n",
        "기계학습 문제는 훈련 데이터를 사용해 학습한다. 더 구체적으로 말하면 훈련 데이터에 대한 손실 함수의 값을 구하고, 그 값을 최대한 줄여주는 매개 변수를 찾아낸다. \n",
        "\n",
        "이렇게 하려면 모든 훈련 데이터를 대상으로 손실 함수 값을 구해야한다. 즉, 훈련 데이터가 100개 있으면 그로부터 계산한 100개의 손실 함수 값들의 합을 지표로 삼는다. \n",
        "\n",
        "지금까지는 데이터 하나에 대한 손실 함수만 생각해왔으니, 이제 훈련 데이터 모두에 대한 손실 함수의 합을 구하는 방법을 생각해보자. 예를 들어 교차 엔트로피 오차는 [식 4.3]처럼 된다. \n",
        "\n",
        "[식 4.3.]\n",
        "\n",
        "\\begin{equation*}\n",
        "E = - \\frac{1}{N} \\sum_{n} \\sum_{k} t_{nk} log y_{nk}\n",
        "\\end{equation*}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1XqBZYNrRuLZ",
        "colab_type": "text"
      },
      "source": [
        "이때 데이터가 N개라면, _t_<sub>nk</sub>는 _n_ 번째 데이터의 _k_ 번째 값을 의미한다. (_y_<sub>nk</sub>는 신경망의 출력, _t_<sub>nk</sub>는 정답 레이블이다)\n",
        "\n",
        "수식이 좀 복잡해 보이지만 데이터 하나에 대한 손실 함수인 [식 4.2]를 단순히 N개의 데이터로 확장했을 뿐이다. 다만 마지막에 N으로 나누어 정규화하고 있다. _N_ 으로 나눔으로써 '평균 손실 함수'를 구하는 것이다. 이렇게 평균을 구해 사용하면 훈련 데이터 개수와 관계없이 언제든 통일된 지표를 얻을 수 있다. (훈련 데이터가 1,000개든, 10,000개든 상관없이 평균 손실 함수를 구할 수 있다. )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yXG_yay6SSf_",
        "colab_type": "text"
      },
      "source": [
        "MNIST데이터셋은 훈련 데이터가 60,000개였다. 그래서 모든 데이터를 대상으로 손실 함수의 합을 구하려면 시간이 좀 걸린다. 더 나아가 빅데이터의 수준이 되면 그 수는 수백만에서 수천만도 넘는 거대한 값이 되기도 한다. 이 많은 데이터를 대상으로 일일이 손실 함수를 계산하는 것은 현실적이지 않다. 이런 경우 데이터 일부를 추려 전체의 '근사치'로 이용할 수 있다. 신경망 학습에서도 훈련 데이터로부터 일부만 골라 학습을 수행한다. 이 일부를 **미니배치**mini-batch라고 한다. 가령 60,000장의 훈련 데이터 중에서 100장을 무작위로 뽑아 그 100장만을 사용하여 학습한 것이다. 이러한 학습 방법을 **미니배치 학습**이라고 한다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A8MiVgsEUX2y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0d43a628-294a-42ce-d52e-3e099ad12e08"
      },
      "source": [
        "!git clone https://github.com/WegraLee/deep-learning-from-scratch.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'deep-learning-from-scratch' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kbdZX_tAUtoD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "775eaad4-7215-40d0-8c2a-3eb85bdf3416"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "deep-learning-from-scratch  sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F_PMSkCTalcd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1QlGNiICaoha",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.chdir(\"./deep-learning-from-scratch\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BB_d6Z19axhD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "fdf15450-7328-465b-a5c6-21a91ce5bf06"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1_vs_2.png  ch03  ch06\tcommon\t\t equations_and_figures.zip  README.md\n",
            "ch01\t    ch04  ch07\tcover_image.jpg  LICENSE.md\n",
            "ch02\t    ch05  ch08\tdataset\t\t map.png\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rCX-963_TBeL",
        "colab_type": "code",
        "outputId": "9a307e98-b7bd-4597-fdc7-24973612fa75",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "import sys, os\n",
        "sys.path.append(os.pardir)\n",
        "import numpy as np\n",
        "from dataset.mnist import load_mnist\n",
        "\n",
        "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
        "\n",
        "print(x_train.shape) # (60000, 784)\n",
        "print(t_train.shape) # (60000, 10)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Converting train-images-idx3-ubyte.gz to NumPy Array ...\n",
            "Done\n",
            "Converting train-labels-idx1-ubyte.gz to NumPy Array ...\n",
            "Done\n",
            "Converting t10k-images-idx3-ubyte.gz to NumPy Array ...\n",
            "Done\n",
            "Converting t10k-labels-idx1-ubyte.gz to NumPy Array ...\n",
            "Done\n",
            "Creating pickle file ...\n",
            "Done!\n",
            "(60000, 784)\n",
            "(60000, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1gjpothyUlBb",
        "colab_type": "text"
      },
      "source": [
        "앞의 코드에서 MNIST 데이터를 읽은 결과, 훈련 데이터는 60,000개고, 입력 데이터는 784열 (원래는 28 * 28)인 이미지 데이터임을 알 수 있다. 또, 정답 레이블은 10줄짜리 데이터이다. 그래서 앞의 x_train, t_train의 모습은 각각 (60000, 784)와 (60000,10)이 된다. \n",
        "\n",
        "그러면 이 훈련 데이터에서 무작위로 10장만 빼내려면 ?  넘파이의 np.random.choice()함수를 쓰면 다음과 같이 간단히 해결 가능 \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BscpjRWgVRsa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_size = x_train.shape[0]\n",
        "batch_size = 10\n",
        "batch_mask = np.random.choice(train_size, batch_size)\n",
        "\n",
        "x_batch = x_train[batch_mask]\n",
        "t_batch = t_train[batch_mask]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pe2C2SbOVcLR",
        "colab_type": "text"
      },
      "source": [
        "np.random.choice()로는 지정한 범위의 수 중에서 무작위로 원하는 개수만 꺼낼 수 있습니다. 가령 np.random.choice(60000, 10)은 0이상 60000 미만의 수 중에서 무작위로 10개를 골라낸다. \n",
        "\n",
        "다음은 실제로 돌려본 모습이다. 이 함수가 출력한 배열을 미니배치로 뽑아낼 데이터의 인덱스로 사용해도 된다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PYhADUyCZl1w",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "f17c5d8d-2c7a-4522-af1c-f23c0f6142ad"
      },
      "source": [
        "np.random.choice(60000,10)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([53924, 17190, 51610, 52518, 29875, 50233, 24900, 56268,  8458,\n",
              "       10828])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z1QDZsJ-Zow6",
        "colab_type": "text"
      },
      "source": [
        "이제 무작위로 선택한 이 인덱스를 사용해 미니배치를 뽑아내기만 하면 된다. 손실 함수도 이 미니배치로 계산한다. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C-UUcUqLZusi",
        "colab_type": "text"
      },
      "source": [
        "### 4.2.4. (배치용) 교차 엔트로피 오차 구현하기\n",
        "\n",
        "그럼, 미니배치 같은 배치 데이터를 지원하는 교차 엔트로피 오차는 어떻게 구현할까?\n",
        "\n",
        "조금 전에 구현한 교차 엔트로피 오차 (데이터를 하나씩 처리하는 구현)를 조금만 바꿔주면 된다. \n",
        "\n",
        "여기에서는 데이터가 하나인 경우와 데이터가 배치로 묶여 입력될 경우 모두를 처리할 수 있도록 구현한다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o_BS4VYgaACQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cross_entropy_error(y, t):\n",
        "  if y.ndim == 1:\n",
        "    t = t.reshape(1, t.size)\n",
        "    y = y.reshape(1, y.size)\n",
        "    \n",
        "    batch_size = y.shape[0]\n",
        "    return -np.sum(t * np.log(y + 1e-7)) / batch_size"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a8oZYHPraO5X",
        "colab_type": "text"
      },
      "source": [
        "이 코드에서 y는 신경망의 출력,  t는 정답 레이블이다. y가 1차원이라면, 즉 데이터 하나당 교차 엔트로피 오차를 구하는 경우는 reshape 함수로 데이터의 형상을 바꿔준다. 그리고 배치의 크기로 나눠 정규화하고 이미지 1장당 평균의 교차 엔트로피 오차를 계산한다. \n",
        "\n",
        "정답 레이블이 원-핫 인코딩이 아니라 '2'나 '7'의 숫자 레이블로 주어졌을 때의 교차 엔트로피 오차는 다음과 같이 구현할 수 있다. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dPDmtltTaVgW",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YcMAOiNod2zU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cross_entropy_error(y, t):\n",
        "  if y.ndim == 1:\n",
        "    t = t.reshape(1, t.size)\n",
        "    y = y.reshape(1, y.size)\n",
        "    \n",
        "    batch_size = y.shape[0]\n",
        "    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7))/ batch_size"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mlJcZcXBeB66",
        "colab_type": "text"
      },
      "source": [
        "이 구현에서는 원-핫 인코딩일 때 t가 0인 원소는 교차 엔트로피 오차도 0이므로, 그 계산은 무시해도 좋다는 것이 핵심입니다. \n",
        "\n",
        "다시 말하면 정답에 해당하는 신경망의 출력만으로 교차 엔트로피 오차를 계산할 수 있다. 그래서 원-핫 인코딩시 t*np.log(y)였던 부분을 레이블 표현일 때는 np.log(y[np.arange(batch_size),t])로 구현한다. (설명을 간결히 하기 위해 미세한 값 1e-7은 언급하지 않겠다)\n",
        "\n",
        "참고로 np.log(y[np.arange(batch_size), t])를 간단히 설명하자면 아래와 같다. \n",
        "\n",
        "np.arange(batch_size)는 0부터 batch_size -1 까지 배열을 생성한다. \n",
        "\n",
        "즉, batch_size가 5이면 np.arang(batch_size)는 [0, 1, 2, 3, 4]라는 넘파이 배열을 생성한다. \n",
        "\n",
        "t 에는 레이블이 [2, 7, 0, 9, 4]와 같이 저장되어 있으므로 y[np.arange(batch_size), t]는 각 데이터의 정답 레이블에 해당하는 신경망의 출력을 추출한다.\n",
        "**이 예에서는 y[np.arange(batch_size), t]는 [y[0, 2], y[1, 7], y[2, 0], y[3, 9], y[4, 4]]인 넘파이 배열을 생성\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BptOghi8lHiN",
        "colab_type": "text"
      },
      "source": [
        "### 4.2.5. 왜 손실 함수를 설정하는가? \n",
        "\n",
        "_신경망을 학습할 때 정확도를 지표로 삼아서는 안 된다. 정확도를 지표로 하면 매개 변수의 미분이 대부분의 장소에서 0이 되기 때문이다_\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rbybzw44lhNS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}