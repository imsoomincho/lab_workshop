{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "W7_training_neural_net_part2(end).ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gXUE3kxn0FBq",
        "colab_type": "text"
      },
      "source": [
        "## 밑바닥 딥러닝\n",
        "## 2019.08.02 \n",
        "### 기울기, 경사하강법에 대해서는 Winter Workshop 2019, Andrew Ng, Introduction to Machine Learning 자료 참조\n",
        "### 4.4.2. 신경망에서의 기울기 \n",
        "### 4.5. 학습 알고리즘 구현하기\n",
        "### 4.5.1. 2층 신경망 구현하기 \n",
        "### 4.5.2. 미니배치 학습 구현하기\n",
        "### 4.5.3. 시험 데이터로 평가하기\n",
        "### 4.6. 정리 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SwElaPhyPglT",
        "colab_type": "text"
      },
      "source": [
        "[복습]\n",
        "\n",
        "왜 손실함수인가? \n",
        "\n",
        "- 신경망 학습에서는 '최적의 매개변수(가중치와 편향)'을 탐색할 때 손실함수의 값을 가능한 작게 하는 매개변수 값을 찾는다. \n",
        "\n",
        "매개 변수의 미분을 계산하고 그 미분값을 단서로 매개변수 값을 서서히 갱신하는 과정을 반복한다. \n",
        "\n",
        "신경망을 학습할 때 정확도를 지표로 삼아서는 안 된다. 정확도를 지표로 삼을 경우 매개변수의 미분의 대부분의 장소에서 0이 된다. \n",
        "\n",
        "대신 손실함수를 지표로 삼으면 매개변수 값이 조금 변하면 그에 반응하여 연속적으로 변화한다. \n",
        "\n",
        "계단함수의 미분은 대부분의 장소 (0이외의 곳)에서 0이다. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8tXZDqdDP9g0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 그림 4-4 계단함수와 시그모이드 함수: 계단 함수는 대부분의 장소에서 기울기가 0이지만, 시그모이드 함수의 기울기(접선)는 0이 아니다.\n",
        "import numpy as np\n",
        "import matplotlib.pylab as plt\n",
        "\n",
        "def step_function(x):\n",
        "    return np.array(x > 0, dtype=np.int)\n",
        "\n",
        "x = np.arange(-6.0, 6.0, 0.1)\n",
        "y1 = step_function(x)\n",
        "y2 = np.array([1 for _ in range(x.size)])\n",
        "y3 = np.array([0 for _ in range(x.size)])\n",
        "plt.plot(x, y1)\n",
        "plt.plot(x, y2, color='green')\n",
        "plt.plot(x, y3, color='green')\n",
        "plt.scatter([4,-4],[1,0],color='red')\n",
        "\n",
        "plt.ylim(-0.1, 1.1) # y축의 범위 지정\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rIYVg9p2QACd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#시그모이드 함수의 기울기 (접선) = 0이 아님을 확인 가능\n",
        "#출처: https://nbviewer.jupyter.org/github/SDRLurker/deep-learning/blob/master/4%EC%9E%A5.ipynb\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_diff(x):\n",
        "    return sigmoid(x) * (1 - sigmoid(x))\n",
        "\n",
        "def sigmoid_tangent(x): # 접선 ax+b에서 a,b 값을 리턴\n",
        "    return sigmoid_diff(x), sigmoid(x) - sigmoid_diff(x) * x\n",
        "\n",
        "x = np.arange(-6.0, 6.0, 0.1)\n",
        "y1 = sigmoid(x)\n",
        "a2, b2 = sigmoid_tangent(4)\n",
        "y2 = a2 * x + b2\n",
        "a3, b3 = sigmoid_tangent(-4)\n",
        "y3 = a3 * x + b3\n",
        "plt.plot(x, y1)\n",
        "plt.plot(x, y2, color='green')\n",
        "plt.plot(x, y3, color='green')\n",
        "plt.scatter([4,-4],[a2*4+b2,a3*-4+b3],color='red')\n",
        "\n",
        "plt.ylim(-0.1, 1.1) # y축의 범위 지정\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YoctHsONQhqw",
        "colab_type": "text"
      },
      "source": [
        "계단 함수는 한 순간만 변화를 일으키지만, 시그모이드 함수는 출력이 연속적으로 변하고 곡선의 기울기(미분)도 연속적으로 변함. 기울기가 0이 되지 않는 덕분에 신경망이 올바르게 학습된다. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ma8mays9Q6Xj",
        "colab_type": "text"
      },
      "source": [
        "## 수치 미분\n",
        "## 편미분 \n",
        "## 기울기 \n",
        "## 경사법(경사 하강법)\n",
        "\n",
        "-- 복습을 위한 참고 자료 : \n",
        "\n",
        "- Andrew Ng (Introduction to ML, Chapter 02, Coursera)\n",
        "- MIT Deep Learning Lecture (#1) https://www.youtube.com/watch?v=5v1JnYv_yWs\n",
        "- Gradient Descent and Cost Function : [medium](https://towardsdatascience.com/machine-learning-fundamentals-via-linear-regression-41a5d11f5220?fbclid=IwAR3inkeUx3yadKlkau4wUBJlgR1ceO2Pqlz6uVuIKtvDz_xp4-VVEWG8uWc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ENB1Lzm05_N",
        "colab_type": "text"
      },
      "source": [
        "### 4.4.2. 신경망에서의 기울기 \n",
        "\n",
        "신경망 학습에서도 기울기를 구해야한다. 여기서 말하는 기울기는 가중치 매개변수에 대한 손실 함수의 기울기이다. 예를 들어 형상이 2*3,  가중치가 W, 손실함수가  L인 신경망을 생각해보자. 이 경우 경사는 &theta;L/&theta;W 로 나타낼 수 있다. 수식으로는 다음과 같다. \n",
        "\n",
        "\\begin{equation*}\n",
        "W =  \\begin{vmatrix}\n",
        "w_{11} w_{21} w_{31}\\\\\n",
        "w_{12} w_{22} w_{32}\\\n",
        "\\end{vmatrix}\n",
        "\\end{equation*}\n",
        "\n",
        "\n",
        "\\begin{equation*}\n",
        "\\frac{\\partial{L}}{\\partial{W}} =  \\begin{vmatrix}\n",
        "\\frac{\\partial{L}}{\\partial{W_{11}}} \\frac{\\partial{L}}{\\partial{W_{21}}} \\frac{\\partial{L}}{\\partial{W_{31}}}\\\\\n",
        "\\frac{\\partial{L}}{\\partial{W_{12}}} \\frac{\\partial{L}}{\\partial{W_{22}}} \\frac{\\partial{L}}{\\partial{W_{32}}}\\\n",
        "\\end{vmatrix}\n",
        "\\end{equation*}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k6PPAYSI1qjw",
        "colab_type": "text"
      },
      "source": [
        "&theta;L/&theta;W 의 각 원소는 각각의 원소에 관한 편미분이다. \n",
        "\n",
        "예를 들어 1행 1번째 원소인 &theta;L/&theta;W<sub>11</sub>은 w<sub>11</sub>을 조금 변경했을 때 손실 함수 L이 얼마나 변화하느냐를 나타낸다. 여기서 중요한 점은 θL/θW의 형상이 W와 같다는 것이다. 실제로 [식 4.8]에서 W와 &theta;L/&theta;W의 형상은 모두 2*3이다. \n",
        "\n",
        "간단한 신경망을 예로 들어 실제로 기울기를 구하는 코드를 구현해보자. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qd4Op1ui3gO4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys, os \n",
        "sys.path.append(os.pardir)\n",
        "import numpy as np\n",
        "from common.functions import softmax, cross_entropy_error\n",
        "from common.gradient import numerical_gradient\n",
        "\n",
        "class simpleNet:\n",
        "  def __init__(self):\n",
        "    self.W = np.random.randn(2,3) #정규분포로 초기화\n",
        "    \n",
        "  def predict(self, x):\n",
        "    return np.dot(x, self.W)\n",
        "  \n",
        "  \n",
        "  def loss(self, x, t):\n",
        "    z = self.predict(x)\n",
        "    y = softmax(z)\n",
        "    loss = cross_entropy_error(y, t)\n",
        "    \n",
        "    return loss\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lKOInbIr48sr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git clone https://github.com/WegraLee/deep-learning-from-scratch.git"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kiDCR7hS4ZLO",
        "colab_type": "text"
      },
      "source": [
        "여기에서는 common/functions.py에 정의한 softmax와 cross_entropy_error 메서드를 사용한다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YBKE1E1p4feX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zC54G4PN5GGy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.chdir(\"./deep-learning-from-scratch\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ej0rNvJQ5UhP",
        "colab_type": "text"
      },
      "source": [
        "simpleNet 클래스는 형상이 2*3인 가중치 매개변수 하나를 인스턴스 변수로 갖는다. 메서드는 2개인데, 하나는 예측을 수행하는 predict(x)이고, 다른 하나는 손실 함수의 값을 구하는 loss(x, t)이다. 여기에서 인수 x는 입력 데이터, t는 정답 레이블이다. 그럼 simpleNet을 사용해 몇 가지 시험을 해보자. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6h4Wz_8q7nfh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "net = simpleNet()\n",
        "\n",
        "print(net.W) #가중치 매개변수"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dzfdsAI978ly",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = np.array([0.6, 0.9])\n",
        "p = net.predict(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NHvbHAd3_USW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(p)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aFPjuAOm_VC2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.argmax(p)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-sScbv-m_Wde",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "t = np.array([0, 0, 1]) #정답 레이블"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gsbuRmFT_Zga",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "net.loss(x, t)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5LVT7qGh_ad_",
        "colab_type": "text"
      },
      "source": [
        "이어서 기울기를 구해볼까요? 지금까지처럼 numerical_gradient(f, x)를 써서 구하면 된다. (여기에서 정의한 f(W) 함수의 인수 W는 더미<sup>dummy</sup>로 만든 것이다) numerical_gradient(f, x) 내부에서 f(x)를 실행하는데, 그와의 일관성을 위해 f(W)를 정의한 것이다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "06M1vGTD_rpb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def f(W):\n",
        "  return net.loss(x, t)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x9qRnpLG_uV4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dW = numerical_gradient(f, net.W)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DUOjMaG__wXS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(dW)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jTX0wkFy_xCa",
        "colab_type": "text"
      },
      "source": [
        "numerical_gradient(f, x)의 인수 f는 함수, x는 함수f의 인수이다. 그래서 여기에서는 net.W를 인수로 받아 손실 함수를 계산하는 새로운 함수f를 정의했다. 그리고 이 새로 정의한 함수를 numerical_gradient(f, x)에 넘긴다. \n",
        "\n",
        "dW는 numerical_gradient(f, net.W)의 결과로, 그 형상은 2*3의 2차원 배열이다. dW의 내용을 보면, 예를 들어  θL/θW의 θL/θW<sub>11</sub>은 대략 0.2이다. \n",
        "\n",
        "이는 w<sub>11</sub>을 _h_ 만큼 늘리면 손실 함수의 값은 0.2h만큼 증가한다. 마찬가지로 θL/θW<sub>23</sub>은 대략 -0.5이니 θL/θW<sub>23</sub>을 h만큼 늘리면 손실 함수의 값은 0.5h만큼 감소하는 것이다. 그래서 손실 함수를 줄인다는 관점에서는 θL/θW<sub>23</sub>은 양의 방향으로 갱신하고 w<sub>11</sub>은 음의 방향으로 갱신해야함을 알 수 있다. 또, 한 번에 갱신되는 양에는 θL/θW<sub>23</sub>이 w<sub>11</sub>보다 크게 기여한다는 사실도 알 수 있다. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bMpLfUleArTi",
        "colab_type": "text"
      },
      "source": [
        "참고로 이 구현에서는 새로운 함수를 정의하는 데 \"def f(x)..\"문법을 썼는데 파이썬에서는 간단한 함수라면 람다(lambda)기법을 쓰면 더 편하다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bDzpUfRmA1dW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "f = lambda w: net.loss(x, t)\n",
        "dW = numerical_gradient(f, net.W)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mLsbQ9EzA64E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3VmTwwu9A75j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dW"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZF9rVWpA8Sc",
        "colab_type": "text"
      },
      "source": [
        "신경망의 기울기를 구한 다음에는 경사법에 따라 가중치 매개변수를 갱신하기만 하면 된다. 다음 절에서는 2층 신경망을 대상으로 학습 과정 전체를 구현한다. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DUmMr-osBGH-",
        "colab_type": "text"
      },
      "source": [
        "### 4.5. 학습 알고리즘 구현하기\n",
        "\n",
        "신경망 학습에 관한 기본적인 지식에 대해 배웠다. \n",
        "\"손실 함수\", \"미니 배치\", \"기울기\", \"경사 하강법\" 등의 중요한 키워드에 대해 배웠다. \n",
        "\n",
        "신경망 학습의 순서를 다시 짚어보자. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VwvDeKkhBSIX",
        "colab_type": "text"
      },
      "source": [
        "- 전체\n",
        "  - 신경망에는 적응 가능한 가중치와 편향이 있고, 이 가중치와 편향을 훈련 데이터에 적응하도록 조정하는 과정을 '학습'이라 한다. 신경망 학습은 다음과 같이 4단계로 수행한다. \n",
        "  \n",
        "  - 1단계 : 미니배치\n",
        "    훈련 데이터 중 일부를 무작위로 가져온다. 이렇게 선별한 데이터를 미니배치라 하며, 그 미니배치의 손실 함수 값을 줄이는 것이 목표다 \n",
        "    \n",
        "  - 2단계: 기울기 산출\n",
        "    미니배치의 손실 함수 값을 줄이기 위해 각 가중치 매개 변수의 기울기를 구한다. 기울기는 손실 함수의 값을 가장 작게 하는 방향을 제시한다. \n",
        "    \n",
        "   - 3단계: 매개 변수 갱신\n",
        "      가중치 매개변수를 기울기 방향으로 아주 조금 갱신한다. \n",
        "      \n",
        "   - 4단계: 반복\n",
        "   \n",
        "     1~3 단계를 반복한다. \n",
        "\n",
        "\n",
        "이 것이 신경망 학습이 이뤄지는 순서다. 이는 경사 하강법으로 매개 변수를 갱신하는 방법이며, 이때 데이터를 미니배치로 무작위로 선정하기 때문에 **확률적 경사 하강법**(stochastic gradient descent)이라고 부른다. \"확률적으로 무작위로 골라낸 데이터\"에 대해 수행하는 경사 하강법이라는 의미다. \n",
        "대부분의 딥러닝 프레임워크는 확률적 경사 하강법의 영어 머리글자를 딴  SGD라는 함수로 이 기능을 구현한다. \n",
        "\n",
        "실제로 손글씨 숫자를 학습하는 신경망을 구현해보자. \n",
        "여기에서는 2층 신경망(은닉층이 1개인 네트워크)을 대상으로 MNIST 데이터셋을 사용하여 학습을 수행한다. \n",
        "\n",
        "\n",
        "   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CB6UmwOBCXf3",
        "colab_type": "text"
      },
      "source": [
        "### 4.5.1. 2층 신경망 클래스 구현하기\n",
        "\n",
        "처음에는 2층 신경망을 하나의 클래스로 구현하는 것부터 시작한다. 이 클래스의 이름은 TwoLayerNet이다. \n",
        "\n",
        "소스코드 : ch04/two_layer_net.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u1lwjzQdGxMI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# https://github.com/WegraLee/deep-learning-from-scratch/blob/master/ch04/two_layer_net.py 소스 참고\n",
        "import numpy as np\n",
        "\n",
        "def sigmoid_grad(x):\n",
        "    return (1.0 - sigmoid(x)) * sigmoid(x)\n",
        "\n",
        "class TwoLayerNet:\n",
        "\n",
        "    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n",
        "        # 가중치 초기화\n",
        "        self.params = {}\n",
        "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
        "        self.params['b1'] = np.zeros(hidden_size)\n",
        "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
        "        self.params['b2'] = np.zeros(output_size)\n",
        "\n",
        "    def predict(self, x):\n",
        "        W1, W2 = self.params['W1'], self.params['W2']\n",
        "        b1, b2 = self.params['b1'], self.params['b2']\n",
        "    \n",
        "        a1 = np.dot(x, W1) + b1\n",
        "        z1 = sigmoid(a1)\n",
        "        a2 = np.dot(z1, W2) + b2\n",
        "        y = softmax(a2)\n",
        "        \n",
        "        return y\n",
        "        \n",
        "    # x : 입력 데이터, t : 정답 레이블\n",
        "    def loss(self, x, t):\n",
        "        y = self.predict(x)\n",
        "        \n",
        "        return cross_entropy_error(y, t)\n",
        "    \n",
        "    def accuracy(self, x, t):\n",
        "        y = self.predict(x)\n",
        "        y = np.argmax(y, axis=1)\n",
        "        t = np.argmax(t, axis=1)\n",
        "        \n",
        "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
        "        return accuracy\n",
        "        \n",
        "    # x : 입력 데이터, t : 정답 레이블\n",
        "    def numerical_gradient(self, x, t):\n",
        "        loss_W = lambda W: self.loss(x, t)\n",
        "        \n",
        "        grads = {}\n",
        "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
        "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
        "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
        "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
        "        \n",
        "        return grads\n",
        "    \n",
        "    def gradient(self, x, t):\n",
        "        W1, W2 = self.params['W1'], self.params['W2']\n",
        "        b1, b2 = self.params['b1'], self.params['b2']\n",
        "        grads = {}\n",
        "        \n",
        "        batch_num = x.shape[0]\n",
        "        \n",
        "        # forward\n",
        "        a1 = np.dot(x, W1) + b1\n",
        "        z1 = sigmoid(a1)\n",
        "        a2 = np.dot(z1, W2) + b2\n",
        "        y = softmax(a2)\n",
        "        \n",
        "        # backward\n",
        "        dy = (y - t) / batch_num\n",
        "        grads['W2'] = np.dot(z1.T, dy)\n",
        "        grads['b2'] = np.sum(dy, axis=0)\n",
        "        \n",
        "        da1 = np.dot(dy, W2.T)\n",
        "        dz1 = sigmoid_grad(a1) * da1\n",
        "        grads['W1'] = np.dot(x.T, dz1)\n",
        "        grads['b1'] = np.sum(dz1, axis=0)\n",
        "\n",
        "        return grads"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TIUdlIFUGzcT",
        "colab_type": "text"
      },
      "source": [
        "TwoLayerNet 클래스가 사용하는 변수\n",
        "\n",
        "params : 신경망의 매개변수를 보관하는 딕셔너리\n",
        "\n",
        "params['W1'] : 1번째 층의 가중치, params['b1'] : 1번째 층의 편향\n",
        "\n",
        "params['W2'] : 2번째 층의 가중치, params['b2'] : 2번째 층의 편향\n",
        "\n",
        "grads : 기울기를 보관하는 딕셔너리\n",
        "\n",
        "grads['W1'] : 1번째 층의 가중치, grads['b1'] : 1번째 층의 편향\n",
        "\n",
        "grads['W2'] : 2번째 층의 가중치, grads['b2'] : 2번째 층의 편향\n",
        "\n",
        "TwoLayerNet 클래스의 메서드\n",
        "\n",
        "__init__(self, input_size, hidden_size, output_size) : 초기화를 수행\n",
        "\n",
        "input_size: 입력층의 뉴런수, hidden_size: 은닉층의 뉴런수, output_size: 출력층의 뉴런수\n",
        "\n",
        "predict(self, x) : 예측(추론)을 수행. 인수 x는 이미지 데이터\n",
        "\n",
        "loss(self, x, t) : 손실 함수의 값을 구함. x는 이미지 데이터. t는 정답 레이블(나머지 메소드 인수도 동일)\n",
        "\n",
        "accuracy(self, x, t) : 정확도를 구함\n",
        "\n",
        "numerical_gradient(self, x, t) : 가중치 매개변수의 기울기를 구함\n",
        "\n",
        "gradient(self, x, t) : 가중치 매개변수의 기울기를 구함. numerical_gradient의 성능 개선판! (구현은 다음장에서...)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZFhxG8_HB3b",
        "colab_type": "text"
      },
      "source": [
        "### 1층의 매개변수 예시 \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rWIDtmfLHLh2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "net = TwoLayerNet(input_size=784, hidden_size=100, output_size=10)\n",
        "print(net.params['W1'].shape) # (784, 100)\n",
        "print(net.params['b1'].shape) # (100,)\n",
        "print(net.params['W2'].shape) # (100, 10)\n",
        "print(net.params['b2'].shape) # (10,)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Me3VFSiFHL0J",
        "colab_type": "text"
      },
      "source": [
        "params변수에는 신경망에 필요한 매개변수가 모두 저장된다. \n",
        "\n",
        "그리고 params 변수에 저장된 가중치 매개변수가 예측 처리 (순방향 처리)에서 사용된다. 참고로 예측 처리는 다음과 같이 실행할 수 있다. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y9QwX9mZHdIo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = np.random.rand(100, 784) #더미 입력 데이터 (100장 분량)\n",
        "\n",
        "y = net.predict(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "07u8OVIHHxZz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sigmoid(x):\n",
        "  return 1/ (1 + np.exp(-x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4X7pEV9HjdY",
        "colab_type": "text"
      },
      "source": [
        "grad변수에는 params변수에 대응하는 각 매개변수의 기울기가 저장된다. \n",
        "예를 들어 다음과 같이 numerical_gradient()메서드를 사용해 기울기를 계산하면 grads변수에 기울기 정보가 저장된다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nOjRzA54IBYc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = np.random.rand(100, 784) #더미 입력 데이터 (100장 분량)\n",
        "t = np.random.rand(100, 10) #더미 정답 레이블 (100장 분량)\n",
        "\n",
        "grads = net.numerical_gradient(x, t) #기울기 계산\n",
        "\n",
        "grads['W1'].shape #(784, 100)\n",
        "grads['b1'].shape #(100, )\n",
        "grads['W2'].shape #(100, 10)\n",
        "grads['b2'].shape #(10, )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yR4LsLoDIV_F",
        "colab_type": "text"
      },
      "source": [
        "초기화 메서드 : __init__ (self, input_size, hidden_size, output_size) 메서드 \n",
        "(순서대로 입력층의 뉴런 수, 은닉층의 뉴런 수, 출력층의 뉴런 수)\n",
        "\n",
        "예를 들어 손글씨 숫자 인식에서는 크기가 28*28인 입력 이미지가 총 784개이고, 출력은 10개가 된다. 따라서 input_size = 784, output_size = 10로 지정하고 은닉층의 개수인 hidden_size는 적당한 값을 설정한다. \n",
        "\n",
        "\n",
        "이 초기화 메서드에서는 가중치 매개변수도 초기화/ 가중치 매개변수의 초깃값을 무엇으로 설정하냐가 신경망 학습의 성공을 좌우하기도 한다. 가중치 매개변수 초기화에 대한 내용은 나중에 재방문. \n",
        "\n",
        "\n",
        "\n",
        "우선, 가중치를 정규분포를 따르는 난수, 편향을 0으로  가정하고 초기화함.\n",
        "\n",
        "loss(self, x, t) = 손실 함수 계산\n",
        "\n",
        "numerical_gradient(self, x, t) 메서드는 각 매개변수의 기울기를 계산함, 수치 미분 방식으로 각 매개변수의 손실 함수에 대한 기울기를 계산\n",
        "\n",
        "gradient(self, x, t) 메서드: 다음장에서 재방문/ **오차역전차법**을 사용하여 기울기를 효율적이고 빠르게 계산"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aGrSXkPGJZf-",
        "colab_type": "text"
      },
      "source": [
        "### 4.5.2. 미니배치 학습 구현하기\n",
        "\n",
        "미니배치에 대해 경사법으로 매개변수를 갱신한다. 그럼 TwoLayerNet클래스와 MNIST 데이터셋을 사용하여 학습을 수행한다 (소스 코드: ch04/train_neuralnet.py)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hpjh8F-BJnvR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys, os\n",
        "sys.path.append(os.pardir)\n",
        "import numpy as np\n",
        "from dataset.mnist import load_mnist\n",
        "\n",
        "\n",
        "# 데이터 읽기\n",
        "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
        "\n",
        "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
        "\n",
        "# 하이퍼파라미터\n",
        "iters_num = 10000  # 반복 횟수를 적절히 설정한다.\n",
        "train_size = x_train.shape[0]\n",
        "batch_size = 100   # 미니배치 크기\n",
        "learning_rate = 0.1\n",
        "\n",
        "train_loss_list = []\n",
        "#이후 추가\n",
        "#이후 추가\n",
        "\n",
        "# 1에폭당 반복 수\n",
        "#이후 추가\n",
        "\n",
        "for i in range(iters_num):\n",
        "    # 미니배치 획득\n",
        "    batch_mask = np.random.choice(train_size, batch_size)\n",
        "    x_batch = x_train[batch_mask]\n",
        "    t_batch = t_train[batch_mask]\n",
        "    \n",
        "    # 기울기 계산\n",
        "    #grad = network.numerical_gradient(x_batch, t_batch)\n",
        "    grad = network.gradient(x_batch, t_batch)\n",
        "    \n",
        "    # 매개변수 갱신\n",
        "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
        "        network.params[key] -= learning_rate * grad[key]\n",
        "    \n",
        "    # 학습 경과 기록\n",
        "    loss = network.loss(x_batch, t_batch)\n",
        "    train_loss_list.append(loss)\n",
        "\n",
        "    # 1에폭당 정확도 계산\n",
        "    #이후 추가"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LBZ2rpU5JtBZ",
        "colab_type": "text"
      },
      "source": [
        "여기서는 미니배치 크기를 100으로 설정함 (즉, 매번 60,000개의 훈련 데이터에서 임의로 100개의 데이터 (이미지, 정답)를 추려낸다)\n",
        "\n",
        "그런 다음 그 100개의 미니배치를 대상으로 확률적 경사 하강법을 수행해 매개변수를 갱신한다. \n",
        "\n",
        "경사법에 의한 갱신 횟수 (반복 횟수)를 10,000번으로 설정하고, 갱신할 때마다 훈련 데이터에 대한 손실 함수를 계산하고, 그 값을 배열에 추가한다. \n",
        "\n",
        "이 손실 함수의 값이 변화하는 추이를 그래프로 나타내면 [그림 4-11]처럼 된다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NCAQusi6KGTj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XYh8H47jJ_xo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 그림 4-11 손실 함수의 추이: 위쪽은 10,000회 반복까지의 추이, 아래쪽은 1,000회 반복까지의 추이\n",
        "f, (ax1, ax2) = plt.subplots(2, 1)\n",
        "x = np.array(range(iters_num))\n",
        "ax1.plot(x, train_loss_list, label='loss')\n",
        "ax1.set_xlabel(\"iteration\")\n",
        "ax1.set_ylabel(\"loss\")\n",
        "ax1.set_ylim(0, 3.0)\n",
        "ax2.plot(x[:1000], train_loss_list[:1000], label='loss')\n",
        "ax2.set_xlabel(\"iteration\")\n",
        "ax2.set_ylabel(\"loss\")\n",
        "ax2.set_ylim(0, 3.0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O6EpH3t5KN4U",
        "colab_type": "text"
      },
      "source": [
        "[그림 4-11]를 보면 학습 횟수가 늘어가면서 손실 함수의 값이 줄어든다.  =신경망의 가중치 매개변수가 서서히 데이터에 적응하고 있음을 의미\n",
        "\n",
        "이는신경망의 학습이 잘 되고 있다는 뜻이다.  \n",
        "\n",
        " = 데이터를 반복해서 학습함으로써 최적 가중치 매개변수로 서서히 다가서고 있다!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ih_-RmF1MbGB",
        "colab_type": "text"
      },
      "source": [
        "### 4.5.3. 시험 데이터로 평가하기\n",
        "\n",
        "[그림 4-11]의 결과에서 학습을 반복함으로써 손실 함수의 값이 서서히 내려가는 것을 확인해보았다. \n",
        "\n",
        "이때의 손실 함수의 **값**이란, 정확히는 '훈련 데이터의 미니배치에 대한 손실 함수'의 값이다. 훈련 데이터의 손실 함수 값이 작아지는 것은 신경망이 잘 학습하고 있다는 방증이지만 이 결과만으로는 다른 데이터셋에도 비슷한 실력을 발휘할지는 확실하지 않다!\n",
        "\n",
        "신경망 학습에서는 훈련 데이터 외의 데이터를 올바르게 인식하는지를 확인해야한다. (즉, **오버피팅**을 일으키지 않는지를 확인해야한다)\n",
        "\n",
        "**오버피팅**이 되었다는 것은 훈련데이터에 포함된 이미지만 제대로 구분하고, 그렇지 않은 이미지는 식별할 수 없다는 뜻이다. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NAuGAoh_NKwe",
        "colab_type": "text"
      },
      "source": [
        "시험 데이터로 신경망이 훈련 데이터에 포함되지 않았던 *새* 데이터에도 적용 가능한지 확인해보자. 여기에서는 1 에폭 별로 훈련 데이터와 시험 데이터에 대한 정확도를 기록한다. \n",
        "\n",
        "**에폭(epoch)** : 하나의 단위, 1 에폭은 학습에서 훈련 데이터를 모두 소진했을 때의 횟수에 해당한다. 예컨데 훈련 데이터 10,000개를 100개의 미니배치로 학습할 경우, 확률적 경사 하강법을 100회 반복하면 모든 훈련 데이터를 '소진'한 게 된다. 이 경우 100회가 1 에폭이 된다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NTX_jBjkNjth",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 데이터 읽기\n",
        "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
        "\n",
        "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
        "\n",
        "# 하이퍼파라미터\n",
        "iters_num = 10000  # 반복 횟수를 적절히 설정한다.\n",
        "train_size = x_train.shape[0]\n",
        "batch_size = 100   # 미니배치 크기\n",
        "learning_rate = 0.1\n",
        "\n",
        "train_loss_list = []\n",
        "train_acc_list = []\n",
        "test_acc_list = []\n",
        "\n",
        "# 1에폭당 반복 수\n",
        "iter_per_epoch = max(train_size / batch_size, 1)\n",
        "\n",
        "for i in range(iters_num):\n",
        "    # 미니배치 획득\n",
        "    batch_mask = np.random.choice(train_size, batch_size)\n",
        "    x_batch = x_train[batch_mask]\n",
        "    t_batch = t_train[batch_mask]\n",
        "    \n",
        "    # 기울기 계산\n",
        "    #grad = network.numerical_gradient(x_batch, t_batch)\n",
        "    grad = network.gradient(x_batch, t_batch)\n",
        "    \n",
        "    # 매개변수 갱신\n",
        "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
        "        network.params[key] -= learning_rate * grad[key]\n",
        "    \n",
        "    # 학습 경과 기록\n",
        "    loss = network.loss(x_batch, t_batch)\n",
        "    train_loss_list.append(loss)\n",
        "\n",
        "    # 1에폭당 정확도 계산\n",
        "    if i % iter_per_epoch == 0:\n",
        "        train_acc = network.accuracy(x_train, t_train)\n",
        "        test_acc = network.accuracy(x_test, t_test)\n",
        "        train_acc_list.append(train_acc)\n",
        "        test_acc_list.append(test_acc)\n",
        "        print(\"train acc, test acc | \" + str(train_acc) + \", \" + str(test_acc))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vChaY34IN5oM",
        "colab_type": "text"
      },
      "source": [
        "이 예시에서는 1 에폭마다 모든 훈련 데이터와 시험 데이터에 대한 정확도를 계산하고, 그 결과를 기록한다. 정확도를 1 에폭마다 계산하는 이유는 for문 안에서 매번 계산하기에는 시간이 오래 걸리고, 또 그렇게까지 자주 기록할 필요도 없기 때문이다. 더 큰 관점에서 그 추이를 알 수 있으면 충분하다. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VyLuV-QYOLZf",
        "colab_type": "text"
      },
      "source": [
        "아래 [그림 4-12]에서 훈련 데이터에 대한 정확도를 실선으로, 시험 데이터에 대한 정확도를 점선으로 그렸다. \n",
        "\n",
        "보다시피 에폭이 진행될수록 (학습이 진행될수록) 훈련 데이터와 시험 데이터를 사용하고 평가한 정확도가 모두 좋아지고 있다. \n",
        "\n",
        "또, 두 정확도에는 차이가 없음을 알 수 있다 (거의 겹쳐있음) = 이번 학습에서는 오버피팅이 일어나지 않았다. \n",
        "\n",
        "만약 오버피팅이 일어났다면 어느 순간부터 시험 데이터에 대한 정확도가 떨어지기 시작한다. (= 이럴 때, 학습을 중단하는데 이걸 조기 종료 (early stopping)라고 하며 이후 \"6.4. 바른 학습을 위해\" 장에서 살펴볼 \"가중치 감소\", \"드롭 아웃\"과 함께 배울 대표적인 오버피팅 예방책이다 )"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5DVJ10cbOFOF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 그림 4-12 훈련 데이터와 시험 데이터에 대한 정확도 추이\n",
        "markers = {'train': 'o', 'test': 's'}\n",
        "x = np.arange(len(train_acc_list))\n",
        "plt.plot(x, train_acc_list, label='train acc')\n",
        "plt.plot(x, test_acc_list, label='test acc', linestyle='--')\n",
        "plt.xlabel(\"epochs\")\n",
        "plt.ylabel(\"accuracy\")\n",
        "plt.ylim(0, 1.0)\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GH2-NPPwOJw_",
        "colab_type": "text"
      },
      "source": [
        "### 4.6. 정리 \n",
        "\n",
        "이번 장에서 배운 내용\n",
        "\n",
        "- 기계 학습에서 사용하는 데이터셋은 훈련 데이터와 시험 데이터로 나눠 사용한다\n",
        "- 훈련 데이터로 학습한 모델의 범용 능력을 시험 데이터로 평가한다\n",
        "- 신경망 학습은 손실 함수를 지표로, 손실 함수의 값이 작아지는 방향으로 가중치 매개변수를 갱신한다\n",
        "- 가중치 매개변수를 갱신할 때는 가중치 매개변수의 기울기를 이용하고, 기울어진 방향으로 가중치의 값을 갱신하는 작업을 반복한다. \n",
        "- 아주 작은 값을 주었을 때의 차분으로 미분하는 것을 수치 미분이라고 한다. \n",
        "- 수치 미분을 이용해 가중치 매개변수의 기울기를 구할 수 있다. \n",
        "- 수치 미분을 이용한 계산에는 시간이 걸리지만, 그 구현은 간단하다. 한편, 다음 장에서 구현하는 (다소 복잡한) 오차역전파법은 기울기를 고속으로 구할 수 있다. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AzSk3mXNPQ50",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}